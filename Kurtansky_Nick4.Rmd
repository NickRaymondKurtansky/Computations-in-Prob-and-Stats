---
output:
  pdf_document: default
  html_document: default
---

###  HW4: OLS, Logistic regression, Bootstrap

**Due: Friday, 3/23/2018, in class**.

**Note:**  Students are required to submit their homework as both PDF file and hardcopy by the due date of the assignment.
Please turn in code separately and electronically. All electronic submissions should be made via D2L Dropbox and should follow the following naming convention: last name, first name, assignment number, proper extension. So, for example, if John Smith is turning in Homework 1, he would name the file Smith_John1.pdf. The associated code would be Smith_John1.Rmd. If you wish to break up your code into separate files, you may submit them as Smith_John1a.Rmd, Smith_John1b.Rmd, and so on. 
There will be a 20%  penalty per day that your homework is late.  Homework in the wrong format will not be given credit.

**Problem 1**
Consider a linear regression of the form $y=X\beta+e$. 

a. Show that the ordinary least squares (OLS)  coefficient estimator $\hat{\beta}$ can be written as a linear function of the
sample values of $Y$, the $Y_i, ~(i = 1, \ldots, N)$. 

* Handwritten

b. Show that the OLS coefficient estimator $\hat{\beta}$ is the unbiased estimator.

* Handwritten

**Problem 2**
Suppose you generate the following binary data with two predictors $X_1$ (continuous) and $X_2$ (binary). 
```{r}
 set.seed(461)
 x1 = rnorm(1000)           
 x2 = rbinom(1000,1,.5)
z = 1 + 2*x1 + 3*x2      
pr = 1/(1+exp(-z))         
y = rbinom(1000,1,pr)      # bernoulli response variable


 dat = data.frame(y=y,x1=x1,x2=x2)
 glm( y~x1+factor(x2),data=dat,family="binomial")
```

Let 
$\hat{\pi}_i(x_{1i},x_{2i})$ be the predicted probability of $y_i=1$ for the $i^{th}$ sample with the variables $(x_1,x_2)$.  

Compute the following *predicted* probability from a logistic regression model.

```{r}
log.odds <- function(x1, x2){
  1.008 + 2.021*x1 + 2.926*x2
}

odds <- function(x1, x2){
  exp(1.008 + 2.021*x1 + 2.926*x2)
}

probability <- function(x1, x2){
  exp(1.008 + 2.021*x1 + 2.926*x2) / (1 + exp(1.008 + 2.021*x1 + 2.926*x2))
}

# for x1 = 2.3, x2 = 1
log.odds(2.3,1)
odds(2.3,1)
probability(2.3,1)

# for x2 = 2.3, x2 = 0
log.odds(2.3,0)
odds(2.3,0)
probability(2.3,0)
```


$(x_{1i},x_{2i})$|    Predicted Probability 
------------| --------------------------
$(2.3, 1)$  | 0.998126
 $(2.3,0)$ | 0.9965167



**Problem 3**
The data for the aspirin
study were collected  by a controlled, randomized, doubleblind
study. One half of the subjects received aspirin and the other half received a control
substance, or placebo, with no active ingredients. The subjects were randomly assigned to
the aspirin or placebo groups. Both the subjects and the supervising physicians were blind to
the assignments, with the statisticians keeping a secret code of who received which substance.


The summary statistics in the study are simple:

Group|heart attacks | subjects
-------------|---|---
aspirin group| 104| 11037
placebo group| 189| 11034

 The
ratio of the two rates (relative risk) is

$\hat{\theta} =\frac{104/11037} {189/11034}= 0.55$.


It suggests that the aspirin-takers only have 55% as many as heart attacks as placebo-takers.

The question is how do we know that $\hat{\theta}$ might not come out much less  favorably if the
experiment were run again?


To answer this question,  apply the bootstrap method  to the heart attack example.

1.  Create two populations: the first consisting of 104 ones and 10933
zeros, and the second consisting of 189 ones and 10845 zeros.

```{r}
p1 <- c(rep(1, times=104), rep(0, times=10933))
p2 <- c(rep(1, times=189), rep(0, times=10845))

theta.hat <- mean(p1) / mean(p2)
theta.hat
```

2.  Draw with replacement a sample of 11037 items from
the first aspirin-taker population, and a sample of 11034 items from the second placebo-taker population.
Each of these is called a bootstrap sample.

```{r}
p1.star <- sample(p1, size=11037, replace=TRUE)
p2.star <- sample(p2, size=11034, replace=TRUE)
```

3. Derive the bootstrap replicate of $\hat{\theta}:$

$$\hat{\theta}^{\star}=\frac{\mbox{prop. of ones in bootstrap sample \#1}}
{\mbox{prop. of ones in bootstrap sample \#2}} $$

```{r}
thetahat.star <- mean(p1.star) / mean(p2.star)
thetahat.star
```

4. Repeat this process (1-3) a large number of times, say 10000 times, and obtain 10000
bootstrap replicates $\hat{\theta}^{\star}$.

```{r}
B <- 10000
B.thetahat <- rep(NA, times=B)

for(i in 1:B){
    #resample from original sample
  p1.star <- sample(p1, size=11037, replace=TRUE)
  p2.star <- sample(p2, size=11034, replace=TRUE)
  
    #calculate odds ratio from bootstrap sample i
  thetahat.star <- mean(p1.star) / mean(p2.star)
  
    #add bootstrap OR i to the list of all bootstrap ORs
  B.thetahat[i] <- thetahat.star
}
```

5. Obtain a bootstrap estimate of the standard error by computing the standard deviation of the bootstrap replicates.

```{r}
se.thetahat <- sd(B.thetahat)
se.thetahat
```

6. Construct  95% confidence interval using normal method and percentile method.

```{r}
  #normal method
round(c(theta.hat - 1.96*se.thetahat, theta.hat + 1.96*se.thetahat), 2)

  #percentile method
B.thetahat <- sort(B.thetahat)
round(B.thetahat[c(length(B.thetahat)*.025, length(B.thetahat)*.975)],2)
```

7. In the language of
statistical hypothesis testing, can we conclude that aspirin is significantly beneficial for reducing heart attack?

YES! Since the 95% confidence interval for relative risk is strictly less than 1, we conclude that aspirin is significantly beneficial in reducing heart attacks.


